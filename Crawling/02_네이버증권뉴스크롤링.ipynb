{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹페이지 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# 크롬 웹드라이버 객체 생성\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 원하는 페이지로 이동하기\n",
    "url = \"https://finance.naver.com/news/mainnews.naver\"\n",
    "driver.get(url)\n",
    "time.sleep(1)\n",
    "\n",
    "# 브라우저 크기 최대화\n",
    "driver.maximize_window()\n",
    "time.sleep(1)\n",
    "\n",
    "# 현재 페이지의 HTML 소스코드 가져오기\n",
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML 문자열을 파싱하여 BeautifulSoup 객체 생성\n",
    "# 이를 통해 HTML 구조를 탐색하고 데이터를 추출할 수 있음\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# soup : 파싱된 HTML을 담는 BeautifulSoup 객체로, 이를 통해 데이터를 쉽게 추출할 수 있음\n",
    "# html : webdriver에서 받아온 html 소스코드\n",
    "# 'html.parser': BeautifulSoup이 사용할 파서(parser) 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"테슬라 반등했는데 한국은 언제쯤\"…개미들 속탄다 [한경우의 케이스스터디]\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 아티클의 제목 추출\n",
    "title = soup.select_one('.articleSubject').text.strip('\\n')\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://n.news.naver.com/mnews/article/421/0007646791'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 아티클의 링크 추출\n",
    "soup.select_one('.articleSubject>a').attrs[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"'10만전자' 다시 오나…'52주 최고가' 삼성전자, 주가 향방은[종목현미경]\", 'https://n.news.naver.com/mnews/article/421/0007646791', \"삼성전자(005930)가 2분기 '어닝 서프라이즈'에 힘입어 3년 5개월 만에 최고가를 기록했다. 특히 개인 투자자들의 매물이 3년 넘..\", '뉴스1 ', '2024-07-06 00:54:53'], [\"뉴욕증시, '냉온탕' 비농업 고용 지표에 혼조 출발\", 'https://n.news.naver.com/mnews/article/001/0014790810', '진정호 연합인포맥스 특파원 = 뉴욕증시가 미국 6월 비농업 부문 고용 결과를 받아본 뒤 혼조 양상을 보이고 있다. 고용은 예상치를 웃돌..', '연합뉴스 ', '2024-07-06 00:14:35'], ['MBK파트너스, 헬스케어 힘주나…日 제약사에 3조 베팅 이유는[주간사모펀드]', 'https://n.news.naver.com/mnews/article/629/0000302170', 'JKL파트너스, 티웨이항공 엑시트 추진 공무원연금공단, MBK파트너스·IMM PE 등 사모대체 블라인드 펀드 위탁운용사 선정 국내 사모..', '더팩트 ', '2024-07-06 00:01:48']]\n"
     ]
    }
   ],
   "source": [
    "# 모든 아티클 목록으로 반복하기\n",
    "articles = soup.select('.block1')\n",
    "article_list = []\n",
    "for article in articles:\n",
    "    # 제목, 상세페이지url, 내용, 언론사, 날짜 가져오기\n",
    "    title = article.select_one(\".articleSubject\").text.strip()\n",
    "    url = article.select_one(\".articleSubject>a\").attrs[\"href\"]\n",
    "    summary = article.select_one(\".articleSummary\").contents[0].strip()\n",
    "    press = article.select_one(\".press\").text\n",
    "    date = article.select_one(\".wdate\").text\n",
    "    article_list.append([title, url, summary, press, date])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터프레임으로 만들기\n",
    "df = pd.DataFrame(article_list,\n",
    "             columns = ['title','url','summary','press','date'])\n",
    "\n",
    "# csv 파일로 저장하기\n",
    "df.to_csv('20240706_네이버증권뉴스.csv', index=False)\n",
    "\n",
    "# 저장한 csv파일을 읽어와 확인하기\n",
    "pd.read_csv('20240706_네이버증권뉴스.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 뉴스기사 날짜 입력받기\n",
    "input_date = input('검색할 날짜(yyyy-nn-dd)-->')\n",
    "\n",
    "# 웹드라이버 생성\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "article_list = []\n",
    "for page in range(1,4):\n",
    "    time.sleep(2)\n",
    "    # 웹페이지 접근\n",
    "    url = f\"https://finance.naver.com/news/mainnews.naver?date={input_date}&page={page}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # HTML 문자열을 파싱하여 BeautifulSoup 객체 생성\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 모든 아티클 리스트 생성하기\n",
    "    articles = soup.select('.block1')    \n",
    "    for article in articles:\n",
    "        # 제목, 상세페이지url, 내용, 언론사, 날짜 가져오기\n",
    "        title = article.select_one(\".articleSubject\").text.strip()\n",
    "        url = article.select_one(\".articleSubject>a\").attrs[\"href\"]\n",
    "        summary = article.select_one(\".articleSummary\").contents[0].strip()\n",
    "        press = article.select_one(\".press\").text\n",
    "        date = article.select_one(\".wdate\").text\n",
    "        article_list.append([title, url, summary, press, date])\n",
    "\n",
    "# 파일로 저장하기\n",
    "df = pd.DataFrame(article_list,\n",
    "             columns = ['title','url','summary','press','date'])\n",
    "df.to_csv('20240706_네이버증권뉴스.csv', index=False)\n",
    "# pd.read_csv('20240706_네이버증권뉴스.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마지막 페이지까지 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 뉴스기사 날짜 입력받기\n",
    "input_date = input('검색할 날짜(yyyy-nn-dd)-->')\n",
    "\n",
    "# 웹드라이버 생성\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "article_list = []\n",
    "page = 1\n",
    "while True:\n",
    "    time.sleep(2)\n",
    "    # 웹페이지 접근\n",
    "    url = f\"https://finance.naver.com/news/mainnews.naver?date={input_date}&page={page}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    driver.maximize_window()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # HTML 문자열을 파싱하여 BeautifulSoup 객체 생성\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 모든 아티클 리스트 생성하기\n",
    "    articles = soup.select('.block1')    \n",
    "    for article in articles:\n",
    "        # 제목, 상세페이지url, 내용, 언론사, 날짜 가져오기\n",
    "        title = article.select_one(\".articleSubject\").text.strip()\n",
    "        url = article.select_one(\".articleSubject>a\").attrs[\"href\"]\n",
    "        summary = article.select_one(\".articleSummary\").contents[0].strip()\n",
    "        press = article.select_one(\".press\").text\n",
    "        date = article.select_one(\".wdate\").text\n",
    "        article_list.append([title, url, summary, press, date])\n",
    "    \n",
    "    # pgRR 요소가 있으면 page에 1 더하기, 없으면 반복 중단\n",
    "    if not soup.select_one('.pgRR'): break\n",
    "    else: page+=1\n",
    "\n",
    "\n",
    "# 파일로 저장하기\n",
    "file_name = f'네이버증권뉴스_{input_date.replace('-','')}' # 파일명 생성\n",
    "\n",
    "df = pd.DataFrame(article_list,\n",
    "             columns = ['title','url','summary','press','date']) # 데이터프레임 생성\n",
    "\n",
    "df.to_csv(file_name+'.csv', index=False) # csv파일로 저장\n",
    "\n",
    "import openpyxl \n",
    "df.to_excel(file_name+'.xlsx', index=False) # excel 파일로 저장\n",
    "\n",
    "driver.quit() # 웹드라이브 종료"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
